{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5229577b",
   "metadata": {},
   "source": [
    "*All content and data presented in the articles on this platform are sourced from the comprehensive boxset titled “Market Risk Analysis” by Carol Alexander. The author of the articles acknowledges and respects the intellectual property rights and copyrights held by the original author of the boxset. The purpose of sharing this information is solely for educational and informational purposes, and no infringement of intellectual property rights is intended.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f3a532",
   "metadata": {},
   "source": [
    "# Variance and Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8528f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from scipy.optimize import minimize\n",
    "import plotly.graph_objects as go\n",
    "import plotly.io as pio\n",
    "import pathlib\n",
    "import sys\n",
    "utils_path = pathlib.Path().absolute().parent.parent\n",
    "sys.path.append(utils_path.__str__())\n",
    "import utils.layout  as lay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc430d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "pio.templates.default = 'simple_white+blog_mra'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed6b900",
   "metadata": {},
   "source": [
    "## Variance and volatility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d01e462",
   "metadata": {},
   "source": [
    "In finance, the term *volatility* $\\sigma$ is measure of dispersion defined the annualized standard deviation of the returns of an investment. Since the standard deviation of monthly returns is not comparable in scale with the standard deviation of daily returns, annualizing both measures is required.\n",
    "In fact, the *square root of time rule* indicates that the *h*-period log return is $\\sqrt{h}$ time the standard deviation of one-period log return:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{i} = \\sqrt{VAR(R_{i})} = \\sqrt{\\frac{k}{n}\\sum_{j=1}^{t}(R_{i, j} - \\bar{R_{i}})^2} \n",
    "\\end{equation}\n",
    "\n",
    "where $k$ is the annualization factor e.g. $k$=250, 52, 12 for daily, weekly and monthly returns, respectively. \n",
    "\n",
    "For that to be true, we need to assume that the one-period returns generating process is i.i.d., which implies a constant volatility assumption. Altough this requisite is not realistic in most financial time series, calculating assets risk by annualizing the standard deviation has become market convention among finance practitioners.\n",
    "\\\n",
    "To deviate from the classic case, we could drop the i.i.d. assumption and add positive or negative autocorrelation in our returns series. Now, we assume an AR(1) autoregressive process of the form\n",
    "\n",
    "\\begin{equation}\n",
    "r_{t} = \\alpha + \\theta r_{t-1} + \\epsilon, \\quad \\epsilon \\sim i.i.d. (0, \\sigma^2) \\quad |\\theta | < 1\n",
    "\\end{equation}\n",
    "\n",
    "where $\\theta$ is the autocorrelation coefficient.\n",
    "\n",
    "In this setup, the derived scaling factor would assume the form:\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{k} = AR(1) Scale Factor = \\left(h+2\\frac{\\theta}{(1-\\theta)^2}\\left[(h-1)(1-\\theta)-\\theta(1-\\theta^{k-1})\\right]\\right)^{1/2}\n",
    "\\end{equation}\n",
    "\n",
    "and the annualization of the standard deviation can be done, as before, as\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{year} =  \\hat{k} \\sigma_{day}\n",
    "\\end{equation}\n",
    "\n",
    "with $h$ = 250. \\\n",
    "Note that the second term in Equation (3) is positive if and only if $ \\theta $ is \n",
    "positive. In other words, positive serial correlation leads to a larger volatility estimate and\n",
    "negative serial correlation leads to lower volatility estimate, compared with the i.i.d. case.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83c94c0",
   "metadata": {},
   "source": [
    "### *Remarks*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84480f49",
   "metadata": {},
   "source": [
    "* Volatility is unobservable; we can only estimate and forecast volatility. There is no absolute \"true\" volatility measure.\n",
    "* Volatility captures only the dispersion of the returns distribution (second moment) when normality is assumed; hence, it does not provide a full description of the risks that are taken by the investments unless we assume the investment returns are normally distributed.\n",
    "* The ‘true’ variance and covariance depend on the model. As a result there is a considerable degree of model risk inherent in the construction of a covariance or correlation\n",
    "matrix. That is, very different results can be obtained using two different statistical\n",
    "models even when they are based on exactly the same data.\n",
    "* The estimates of the true covariance matrix are subject to sampling error. Even when two\n",
    "analysts use the same model to estimate a covariance matrix their estimates will differ\n",
    "if they use different data to estimate the matrix. Both changing the sample period and\n",
    "changing the frequency of the observations will affect the covariance matrix estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "691c8c7a",
   "metadata": {},
   "source": [
    "### *Examples*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b376f7",
   "metadata": {},
   "source": [
    "### (I) *Volatility from standard deviation*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95c4f33",
   "metadata": {},
   "source": [
    "<em> Assume returns are generated by an i.i.d. process. \\\n",
    "(a) The variance of daily returns is 0.001. Assuming 250 risk days per year, what is the\n",
    "volatility? \\\n",
    "(b) The volatility is 36%. What is the standard deviation of weekly returns?</em> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cebd4e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solution A -> Volatility = 0.5\n",
      "Solution B -> Std Dev = 0.05\n"
     ]
    }
   ],
   "source": [
    "sol_a = np.sqrt(0.001*250)\n",
    "sol_b = 0.36/np.sqrt(52)\n",
    "print(\"Solution A -> Volatility = {}\".format(sol_a))\n",
    "print(\"Solution B -> Std Dev = {}\".format(round(sol_b, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959312c4",
   "metadata": {},
   "source": [
    "### *(2) Volatility when returns are autocorrelated*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a707fa",
   "metadata": {},
   "source": [
    "*Monthly returns on a hedge fund over the last three years have a standard deviation of 5%.\n",
    "Assume the returns are i.i.d. What is your volatility estimate? Now suppose you discover\n",
    "that the returns have been smoothed before reporting them to the investors. In fact, the\n",
    "returns are autocorrelated with autocorrelation 0.25. What is your volatility estimate now?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbcb9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equation 3\n",
    "def ar1_scale_factor(n, theta):\n",
    "    \"\"\"\n",
    "    n = number of returns\n",
    "    theta = autocorrelation coefficient\n",
    "    \"\"\"\n",
    "    return (n+2*((theta/(1-theta)**2)*((n-1)*(1-theta)-theta*(1-theta**(n-1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5143f862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Volatility IID: 0.1732\n",
      "Volatility AR1: 0.2186\n"
     ]
    }
   ],
   "source": [
    "std_monthly_rets = 0.05\n",
    "n_rets = 12\n",
    "volatility_iid = std_monthly_rets*np.sqrt(n_rets)\n",
    "autocorr = 0.25 \n",
    "scaling_factor = ar1_scale_factor(n_rets, autocorr)\n",
    "volatility_ar1 = std_monthly_rets*np.sqrt(scaling_factor)\n",
    "print(\"Volatility IID: {}\".format(round(volatility_iid, 4)))\n",
    "print(\"Volatility AR1: {}\".format(round(volatility_ar1,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0456014",
   "metadata": {},
   "source": [
    "## Covariance and correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50fcb5d",
   "metadata": {},
   "source": [
    "The *covariance* between two returns is the first central moment of their joint density function and it can take any real number. \n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{ij} = \\frac{1}{N} \\sum_{i=1}^{t} (r_{i} - \\bar{r_{i}}) (r_{j} - \\bar{r_{j}})\n",
    "\\end{equation}\n",
    "\n",
    "Given that covariances vary in scale with the size of the returns, instead of time like for volatility, we can obtain a standardize measure dividing the covariance of two returns by the product of the their standard deviations. This standardize measure, namely *correlation*, lies between -1 and +1.\n",
    "Note that zero correlation implies independence only is the two returns have a bivariate normal distribution.\n",
    "\n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{ij} = \\frac{\\sigma_{ij}}{\\sigma_{i} \\sigma_{j}}\n",
    "\\end{equation}\n",
    "\n",
    "Finance practitioners have always used Pearson's correlation to measure of dependency. However, the metric is not appropriate when two returns have an elliptical joint distribution. Moreover, the assumption of multivariate normal i.i.d returns is not empirically justified, which makes the results inaccurate. In this case, using copula function as measure of dependency improves substantially the accuracy of our results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251a502b",
   "metadata": {},
   "source": [
    "### *Example: portfolio variance*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3894de",
   "metadata": {},
   "source": [
    "*A portfolio has 1 million invested in asset 1, 2 millions invested in asset 2 and 3\n",
    "millions invested in asset 3. The annual volatilities and correlations of the asset returns are given in\n",
    "the below table. Find the portfolio volatility.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769280fe",
   "metadata": {},
   "source": [
    "$$\\begin{array}{llll}\n",
    "\\hline\n",
    "Asset 1\\ volatility & 0.2 & Asset 1 - Asset 2\\ Correlation & 0.8 \\\\\n",
    "Asset 2\\ volatility & 0.1 & Asset 1 - Asset 3\\ Correlation & 0.5 \\\\\n",
    "Asset 3\\ volatility & 0.15 & Asset 2 - Asset 3\\ Correlation & 0.3 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61816fa",
   "metadata": {},
   "source": [
    "Portfolio weights:\n",
    "$$ w = (\\frac{1}{6}, \\frac{2}{6}, \\frac{3}{6}) $$\n",
    "\n",
    "Portfolio variance:\n",
    "\n",
    "$$ V(R) = w'Vw $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a4e1469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portfolio Variance: 0.013625\n",
      "Portfolio Volatility: 0.11673\n"
     ]
    }
   ],
   "source": [
    "w = np.array([1/6, 2/6, 3/6])\n",
    "V = np.array([[0.04, 0.016, 0.015], [0.016, 0.01, 0.0045,], [0.015, 0.0045, 0.0225]])\n",
    "Ptf_var = w.T.dot(V).dot(w)\n",
    "print(\"Portfolio Variance: {}\".format(Ptf_var))\n",
    "print(\"Portfolio Volatility: {}\".format(round(np.sqrt(Ptf_var), 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd06752",
   "metadata": {},
   "source": [
    "## Scaling Covariance Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f996d",
   "metadata": {},
   "source": [
    "An $h$-day covariance matrix $V_{h}$ is the matrix of variances and covariances of $h$-day returns e.g. h=1 means daily returns. If the returns are i.i.d. and the joint distribution is multivariate normal, then variance and covariance scale with time as \n",
    "\n",
    "\\begin{equation}\n",
    "V = hV_{h}\n",
    "\\end{equation}\n",
    "\n",
    "where $h$ is the frequency of the returns used in the covariance calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08d8819",
   "metadata": {},
   "source": [
    "### *(2) Scaling and decomposing covariance matrix*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783d5613",
   "metadata": {},
   "source": [
    "*The volatilities and correlation between returns on three assets are shown in the above table. As\n",
    "usual, the volatilities are quoted as annualized percentages. Calculate the annual covariance\n",
    "matrix. Then assuming the returns are multivariate normal i.i.d. and assuming 250 trading\n",
    "days per year, derive from this the 10-day covariance matrix, i.e. the matrix of covariance\n",
    "of 10-day returns*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74664d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_correlation_matrix(cross_corr):\n",
    "    \"\"\"\n",
    "    Create a 3x3 correlation matrix with 1s on the diagonal and\n",
    "    fill in the upper and lower triangular parts with cross-correlations.\n",
    "\n",
    "    Args:\n",
    "        cross_corr (numpy.ndarray): A 3x1 array of cross-correlations between three assets.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A 3x3 correlation matrix.\n",
    "    \"\"\"\n",
    "    n = cross_corr.shape[0]  \n",
    "    corr_matrix = np.zeros((n, n))\n",
    "    np.fill_diagonal(corr_matrix, 1)\n",
    "\n",
    "    for i in range(n):\n",
    "        for j in range(i + 1, n):\n",
    "            corr_matrix[i, j] = cross_corr[i, 0]\n",
    "            corr_matrix[j, i] = cross_corr[i, 0]\n",
    "\n",
    "    return corr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e0d7b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 10\n",
    "vols = np.array([0.2, 0.1, 0.15])\n",
    "D = np.diag(vols)\n",
    "correlations = np.array([[0.8], [0.5],  [0.3]])\n",
    "C = create_correlation_matrix(correlations)\n",
    "V_annual = D.dot(C).dot(D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ef3a27",
   "metadata": {},
   "source": [
    "$$ V = DCD $$\n",
    "\n",
    "$$ 250-day\\ Covariance\\ Matrix $$\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "0.04 & 0.02 & 0.02 \\\\\n",
    "0.02 & 0.01 & 0.01 \\\\\n",
    "0.02 & 0.01 & 0.02 \\\\\n",
    "\\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e1313eb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "V_scaled = V_annual / (250/h) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f93307",
   "metadata": {},
   "source": [
    "$$ 10-day\\ Covariance\\ Matrix $$\n",
    "$$ \n",
    "\\begin{bmatrix}\n",
    "16.00 & 6.40 & 9.60 \\\\\n",
    "6.40 & 4.00 & 3.00 \\\\\n",
    "9.60 & 3.00 & 9.00 \\\\\n",
    "\\end{bmatrix} \\cdot 10^{-4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c7da3a",
   "metadata": {},
   "source": [
    "### Correlation Pitfalls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65502694",
   "metadata": {},
   "source": [
    "Despite its wide adoption among finance practitioners, Pearson's correlation comes with numerous limitations. Embrechts et al. (2002) explain correlation pitfalls in his famous article \"Correlation And Dependence In Risk Management: Properties And Pitfalls\". Here we list some of them:\n",
    "\n",
    "* It is only  linear measure of association that is not flexible enough to capture non-linear dependencies. -> $Cov(X, X^2) = 0$\n",
    "* Correlation is not invariant under transformation of variables -> $Corr(X, Y) \\neq Corr(\\ln{(X)}, \\ln{(Y)})$\n",
    "* Feasible values for correlation depend on the marginal distributions\n",
    "* Perfect positive dependence does not imply a correlation of one. With the lognormal variables above,\n",
    "perfect positive dependence implies a correlation of two-thirds and perfect negative\n",
    "dependence implies a correlation of only −0.09.\n",
    "* Zero correlation does not imply independence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
